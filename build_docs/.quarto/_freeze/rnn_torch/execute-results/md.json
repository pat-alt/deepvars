{
  "hash": "e8d23b4db2ac13a751c587da715b99a4",
  "result": {
    "markdown": "---\ntitle: \"RNN in Torch\"\nauthor: \"Patrick Altmeyer\"\nformat: gfm\n---\n\n## Overview\n\nThis tutorial demonstrates how the Recurrent Neural Network underlying Deep VAR is set up using [torch for R](https://torch.mlverse.org/). It heavily draws on ideas and code presented in this great [tutorial](https://blogs.rstudio.com/ai/posts/2021-03-11-forecasting-time-series-with-torch_2/) from the RStudio AI blog. \n\n## Data input\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_val_test_split <- function(data, train_size=0.8) {\n  N <- nrow(data)\n  end_train <- round(train_size * N)\n  end_val <- end_train + round((N - end_train)/2)\n  data_train <- data[1:end_train,] |> as.matrix()\n  data_val <- data[(end_train+1):end_val,] |> as.matrix()\n  data_test <- data[(end_val+1):N,] |> as.matrix()\n  return(list(train=data_train, val=data_val, test=data_test))\n}\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(deepvars)\nlibrary(data.table)\ndata(\"canada\")\ndt <- data.table(canada)\nvar_cols = colnames(dt)[2:ncol(dt)]\ndt[,(var_cols) := lapply(.SD, function(i) c(0,diff(i))), .SDcols=var_cols]\nsplits_ <- train_val_test_split(dt[,-1], train_size = 0.5)\ndf_train <- splits_$train\ndf_val <- splits_$val\ndf_test <- splits_$test\n```\n:::\n\nRecall that the neural network that we are aiming to build takes as its input its own $p$ lags as well as $p$ lags of all other variables in the system.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\n\ndvar_dataset <- dataset(\n  name = \"dvar_dataset\",\n  \n  initialize = function(X, response, lags, n_ahead, sample_frac = 1) {\n    \n    self$lags <- lags\n    self$n_ahead <- n_ahead\n    self$response <- response\n    self$train_mean <- colMeans(X)\n    self$train_sd <- sapply(1:ncol(X), function(i) sd(X[,i]))\n    self$X <- torch_tensor(t((t(X) - self$train_mean)/self$train_sd)) # of dimension (D x T)\n    \n    n <- dim(self$X)[1] - self$lags - self$n_ahead + 1\n    \n    self$starts <- sort(sample.int(\n      n = n,\n      size = round(n * sample_frac)\n    ))\n    \n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$lags - 1\n    pred_length <- self$n_ahead\n    \n    list(\n      X = self$X[start:end,],\n      y = self$X[(end + 1):(end + pred_length),self$response]\n    )\n    \n  },\n  \n  .length = function() {\n    length(self$starts) \n  }\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nresponse_var_idx <- 1\nlags <- 6\nn_ahead <- 12\ntrain_ds <- dvar_dataset(df_train, response_var_idx, lags, n_ahead = n_ahead, sample_frac = 0.5)\n\nbatch_size <- 30\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds <- dvar_dataset(df_val, response_var_idx, lags, n_ahead=n_ahead, sample_frac = 0.5)\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n\ntest_ds <- dvar_dataset(df_test, response_var_idx, lags, n_ahead=n_ahead)\ntest_dl <- test_ds %>% dataloader(batch_size = 1)\n```\n:::\n\nLet's do a quick sanity check to see if the dimensions check out:\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_ds[1]\n```\n\n::: {.cell-output-stdout}\n```\n$X\ntorch_tensor\n 0.1020 -1.1904  0.9092 -0.4763\n 0.9909  0.5094  1.8195 -0.4147\n 1.1777  1.1057  1.2633  0.2022\n 0.6616 -0.9201  1.6707 -0.4969\n-0.6957 -2.2618 -0.5863  0.5517\n-1.3460 -1.2138  3.2111  1.9088\n[ CPUFloatType{6,4} ]\n\n$y\ntorch_tensor\n-2.0908\n-2.8897\n-2.9660\n-1.4641\n 0.0895\n 1.1206\n 1.4424\n 0.0236\n-0.4609\n 0.1643\n 0.6791\n-0.0035\n[ CPUFloatType{12} ]\n```\n:::\n:::\n\nThis looks like what we expected: `X` is a $(D \\times p)$ tensor and $y$ is just the single output.\n\n::: {.cell}\n\n```{.r .cell-code}\nin_idx <- train_ds$starts[1]:(train_ds$starts[1]+train_ds$lags-1)\nout_idx <- c(max(in_idx)+1,response_var_idx)\nX_train <- t((t(df_train) - colMeans(df_train)) / sapply(1:ncol(df_train), function(i) sd(df_train[,i])))\nX_train[in_idx,]\n```\n\n::: {.cell-output-stdout}\n```\n              e       prod         rw          U\n[1,]  0.1019674 -1.1904140  0.9092028 -0.4763473\n[2,]  0.9908521  0.5093805  1.8195315 -0.4146620\n[3,]  1.1776523  1.1057034  1.2633140  0.2021906\n[4,]  0.6615854 -0.9200533  1.6706661 -0.4969091\n[5,] -0.6956522 -2.2618121 -0.5862564  0.5517404\n[6,] -1.3459703 -1.2137674  3.2111416  1.9088162\n```\n:::\n\n```{.r .cell-code}\nX_train[out_idx[1]:(out_idx[1]+n_ahead-1),out_idx[2]]\n```\n\n::: {.cell-output-stdout}\n```\n [1] -2.090789787 -2.889711226 -2.966011884 -1.464126631  0.089478145\n [6]  1.120601201  1.442417457  0.023572063 -0.460867622  0.164265269\n[11]  0.679070086 -0.003545797\n```\n:::\n:::\n\nWhat about the mini-batch? We see that `X` is of the desired dimension `(batch_size, n_timesteps, num_features)`.\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(train_dl)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nb <- train_dl %>% dataloader_make_iter() %>% dataloader_next()\nb\n```\n\n::: {.cell-output-stdout}\n```\n$X\ntorch_tensor\n(1,.,.) = \n  1.4424 -0.1345 -0.7919 -1.0932\n  0.0236  0.8412  0.4699 -1.0315\n -0.4609  1.5454  0.1831  0.1405\n  0.1643  1.0204 -0.5691  0.4078\n  0.6791 -0.7397 -0.8391 -0.3530\n -0.0035  1.1853 -0.1828 -0.2707\n\n(2,.,.) = \n -2.9660  0.4020  0.2097  3.6360\n -1.4641 -0.1386  0.6799  1.1686\n  0.0895  1.6895 -1.6889 -0.7025\n  1.1206  0.4061 -0.2826 -0.4147\n  1.4424 -0.1345 -0.7919 -1.0932\n  0.0236  0.8412  0.4699 -1.0315\n\n(3,.,.) = \n  1.1206  0.4061 -0.2826 -0.4147\n  1.4424 -0.1345 -0.7919 -1.0932\n  0.0236  0.8412  0.4699 -1.0315\n -0.4609  1.5454  0.1831  0.1405\n  0.1643  1.0204 -0.5691  0.4078\n  0.6791 -0.7397 -0.8391 -0.3530\n\n(4,.,.) = \n  0.6616 -0.9201  1.6707 -0.4969\n -0.6957 -2.2618 -0.5863  0.5517\n -1.3460 -1.2138  3.2111  1.9088\n -2.0908  0.1093  1.7242  1.0247\n -2.8897 -1.1323  0.8589  3.2865\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{12,6,4} ]\n\n$y\ntorch_tensor\nColumns 1 to 10-0.0456  0.8483  0.6847  0.6916  0.8213  0.2048 -0.3137 -0.0030  0.1853  1.1057\n-0.4609  0.1643  0.6791 -0.0035 -0.0456  0.8483  0.6847  0.6916  0.8213  0.2048\n-0.0035 -0.0456  0.8483  0.6847  0.6916  0.8213  0.2048 -0.3137 -0.0030  0.1853\n-1.4641  0.0895  1.1206  1.4424  0.0236 -0.4609  0.1643  0.6791 -0.0035 -0.0456\n 0.8213  0.2048 -0.3137 -0.0030  0.1853  1.1057  0.6218  1.1774  0.5537  0.0394\n 0.1853  1.1057  0.6218  1.1774  0.5537  0.0394 -0.2996  0.4384  0.9216 -0.6563\n-0.3137 -0.0030  0.1853  1.1057  0.6218  1.1774  0.5537  0.0394 -0.2996  0.4384\n 1.4424  0.0236 -0.4609  0.1643  0.6791 -0.0035 -0.0456  0.8483  0.6847  0.6916\n-2.9660 -1.4641  0.0895  1.1206  1.4424  0.0236 -0.4609  0.1643  0.6791 -0.0035\n 0.6916  0.8213  0.2048 -0.3137 -0.0030  0.1853  1.1057  0.6218  1.1774  0.5537\n-2.0908 -2.8897 -2.9660 -1.4641  0.0895  1.1206  1.4424  0.0236 -0.4609  0.1643\n 0.0236 -0.4609  0.1643  0.6791 -0.0035 -0.0456  0.8483  0.6847  0.6916  0.8213\n\nColumns 11 to 12 0.6218  1.1774\n-0.3137 -0.0030\n 1.1057  0.6218\n 0.8483  0.6847\n-0.2996  0.4384\n-0.1092 -0.0666\n 0.9216 -0.6563\n 0.8213  0.2048\n-0.0456  0.8483\n 0.0394 -0.2996\n 0.6791 -0.0035\n 0.2048 -0.3137\n[ CPUFloatType{12,12} ]\n```\n:::\n:::\n\n## Model\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- nn_module(\n  \n  initialize = function(type=\"lstm\", input_size, hidden_size, linear_size, output_size,\n                        num_layers = 2, dropout = 0.25, linear_dropout = 0.25) {\n    \n    self$type <- type\n    self$num_layers <- num_layers\n    self$linear_dropout <- linear_dropout\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    }\n    \n    self$mlp <- nn_sequential(\n      nn_linear(hidden_size, linear_size),\n      nn_relu(),\n      nn_dropout(linear_dropout),\n      nn_linear(linear_size, output_size)\n    )\n    \n  },\n  \n  forward = function(x) {\n    \n    x <- self$rnn(x)\n    x[[1]][ ,-1, ..] %>% \n      self$mlp()\n    \n  }\n  \n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- dim(train_ds[1]$X)[2]\nnet <- model(input_size = D, hidden_size = 32, linear_size = 128, output_size = n_ahead)\ndevice <- torch_device(if (cuda_is_available()) \"cuda\" else \"cpu\")\nnet <- net$to(device = device)\n```\n:::\n\n## Training\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 30\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad() # in\n  output <- net(b$X$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$backward()\n  optimizer$step()\n  \n  loss$item()\n}\n\nvalid_batch <- function(b) {\n  \n  output <- net(b$X$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$item()\n  \n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    loss <- train_batch(b)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n  \n  net$eval()\n  valid_loss <- c()\n  \n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_loss <- c(valid_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n```\n\n::: {.cell-output-stdout}\n```\n\nEpoch 1, training: loss: 0.64473 \n\nEpoch 1, validation: loss: 0.79544 \n\nEpoch 2, training: loss: 0.64353 \n\nEpoch 2, validation: loss: 0.78293 \n\nEpoch 3, training: loss: 0.63548 \n\nEpoch 3, validation: loss: 0.77065 \n\nEpoch 4, training: loss: 0.63260 \n\nEpoch 4, validation: loss: 0.75832 \n\nEpoch 5, training: loss: 0.62000 \n\nEpoch 5, validation: loss: 0.74583 \n\nEpoch 6, training: loss: 0.61640 \n\nEpoch 6, validation: loss: 0.73339 \n\nEpoch 7, training: loss: 0.60513 \n\nEpoch 7, validation: loss: 0.72109 \n\nEpoch 8, training: loss: 0.59723 \n\nEpoch 8, validation: loss: 0.70906 \n\nEpoch 9, training: loss: 0.59636 \n\nEpoch 9, validation: loss: 0.69679 \n\nEpoch 10, training: loss: 0.58609 \n\nEpoch 10, validation: loss: 0.68437 \n\nEpoch 11, training: loss: 0.57969 \n\nEpoch 11, validation: loss: 0.67186 \n\nEpoch 12, training: loss: 0.57611 \n\nEpoch 12, validation: loss: 0.65910 \n\nEpoch 13, training: loss: 0.57321 \n\nEpoch 13, validation: loss: 0.64597 \n\nEpoch 14, training: loss: 0.55939 \n\nEpoch 14, validation: loss: 0.63253 \n\nEpoch 15, training: loss: 0.57057 \n\nEpoch 15, validation: loss: 0.61889 \n\nEpoch 16, training: loss: 0.54712 \n\nEpoch 16, validation: loss: 0.60493 \n\nEpoch 17, training: loss: 0.54523 \n\nEpoch 17, validation: loss: 0.59066 \n\nEpoch 18, training: loss: 0.53944 \n\nEpoch 18, validation: loss: 0.57578 \n\nEpoch 19, training: loss: 0.53803 \n\nEpoch 19, validation: loss: 0.56049 \n\nEpoch 20, training: loss: 0.53603 \n\nEpoch 20, validation: loss: 0.54494 \n\nEpoch 21, training: loss: 0.51970 \n\nEpoch 21, validation: loss: 0.52912 \n\nEpoch 22, training: loss: 0.51931 \n\nEpoch 22, validation: loss: 0.51338 \n\nEpoch 23, training: loss: 0.51753 \n\nEpoch 23, validation: loss: 0.49760 \n\nEpoch 24, training: loss: 0.50235 \n\nEpoch 24, validation: loss: 0.48188 \n\nEpoch 25, training: loss: 0.49555 \n\nEpoch 25, validation: loss: 0.46675 \n\nEpoch 26, training: loss: 0.50679 \n\nEpoch 26, validation: loss: 0.45367 \n\nEpoch 27, training: loss: 0.50799 \n\nEpoch 27, validation: loss: 0.44272 \n\nEpoch 28, training: loss: 0.50215 \n\nEpoch 28, validation: loss: 0.43401 \n\nEpoch 29, training: loss: 0.49051 \n\nEpoch 29, validation: loss: 0.42748 \n\nEpoch 30, training: loss: 0.48860 \n\nEpoch 30, validation: loss: 0.42310 \n```\n:::\n:::\n\n\n## Evaluation\n\nBelow we see the 12-step prediction\n\n::: {.cell}\n\n```{.r .cell-code}\nnet$eval()\n\ntest_preds <- vector(mode = \"list\", length = length(test_dl))\n\ni <- 1\n\ncoro::loop(for (b in test_dl) {\n  \n  input <- b$X\n  output <- net(input$to(device = device))\n  preds <- as.numeric(output)\n  \n  test_preds[[i]] <- preds\n  i <<- i + 1\n  \n})\n\ny_hat <- test_preds[[1]] * train_ds$train_sd[response_var_idx] + train_ds$train_mean[response_var_idx]\ny_hat <- data.table(y_hat)[,id:=1:.N][,variable:=colnames(df_train)[response_var_idx]]\nsetkey(y_hat, id, variable)\n\nlibrary(data.table)\ny_true <- data.table(rbind(df_val,df_test))\ny_true <- y_true[,..response_var_idx]\ny_true[,id:=(-nrow(df_val)-lags+1):(-nrow(df_val)-lags+y_true[,.N])]\ny_true <- melt(y_true, id.vars = \"id\", value.name = \"y\")\nsetkey(y_true, id, variable)\n\ndt_plot <- melt(y_hat[y_true],id.vars = c(\"id\",\"variable\"), variable.name = \"type\")\ndt_plot[type==\"y_hat\" & id==0, value:=dt_plot[type==\"y\" & id==0]$value]\nlibrary(ggplot2)\n\nggplot(dt_plot[id<=n_ahead], aes(x=id, y=value, colour=type)) +\n  geom_line() +\n  geom_point() +\n  scale_color_manual(name=\"Type:\", values=c(\"blue\", \"red\"), labels=c(\"Prediction\", \"Actual\")) +\n  labs(\n    title = sprintf(\"Variable: %s\", colnames(df_train)[response_var_idx]),\n    x = \"Time\",\n    y = \"Value\"\n  )\n```\n\n::: {.cell-output-display}\n![](rnn_torch_files/figure-gfm/unnamed-chunk-11-1.png)\n:::\n:::",
    "supporting": [
      "rnn_torch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}